#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç›˜èµ„æºçˆ¬è™«ç³»ç»Ÿ
æ”¯æŒå¤šä¸ªå¹³å°çš„èµ„æºæ”¶é›†
"""

import asyncio
import csv
import json
import re
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

import httpx
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
import pandas as pd


class NetdiskSpider:
    """ç½‘ç›˜èµ„æºçˆ¬è™«"""

    def __init__(self):
        self.session = None
        self.results = []
        self.platforms_patterns = {
            'quark': [
                r'https?://pan\.quark\.cn/s/[a-zA-Z0-9]+',
                r'å¤¸å…‹ç½‘ç›˜',
                r'å¤¸å…‹é“¾æ¥'
            ],
            'baidu': [
                r'https?://pan\.baidu\.com/s/[a-zA-Z0-9_-]+',
                r'ç™¾åº¦ç½‘ç›˜',
                r'ç™¾åº¦äº‘'
            ],
            'aliyun': [
                r'https?://www\.aliyundrive\.com/s/[a-zA-Z0-9]+',
                r'é˜¿é‡Œäº‘ç›˜',
                r'é˜¿é‡Œç½‘ç›˜'
            ],
            'tianyi': [
                r'https?://cloud\.189\.cn/[tw]/[a-zA-Z0-9]+',
                r'å¤©ç¿¼äº‘ç›˜',
                r'å¤©ç¿¼ç½‘ç›˜'
            ],
            '123pan': [
                r'https?://www\.123pan\.com/s/[a-zA-Z0-9_-]+',
                r'123ç½‘ç›˜',
                r'123äº‘ç›˜'
            ]
        }

    def extract_netdisk_info(self, text: str) -> List[Dict]:
        """ä»æ–‡æœ¬ä¸­æå–ç½‘ç›˜ä¿¡æ¯"""
        results = []

        for platform, patterns in self.platforms_patterns.items():
            # æŸ¥æ‰¾é“¾æ¥
            for pattern in patterns:
                if 'http' in pattern:
                    matches = re.findall(pattern, text, re.IGNORECASE)
                    for match in matches:
                        # å°è¯•æå–å¯†ç /æå–ç 
                        password = self._extract_password(text, match)

                        results.append({
                            'platform': platform,
                            'url': match,
                            'password': password,
                            'title': self._extract_title(text, match),
                            'size': self._extract_size(text),
                            'source': 'crawler',
                            'crawl_time': datetime.now().isoformat()
                        })

        return results

    def _extract_password(self, text: str, url: str) -> str:
        """æå–å¯†ç /æå–ç """
        # å¸¸è§çš„å¯†ç æ¨¡å¼
        patterns = [
            r'(?:æå–ç |å¯†ç |pwd|password)[:ï¼š]\s*([a-zA-Z0-9]{4,6})',
            r'([a-zA-Z0-9]{4,6})\s*(?:æå–ç |å¯†ç )',
            r'æå–ç \s*([a-zA-Z0-9]{4,6})',
            r'å¯†ç \s*([a-zA-Z0-9]{4,6})',
        ]

        # åœ¨URLé™„è¿‘å¯»æ‰¾å¯†ç 
        url_index = text.find(url)
        if url_index != -1:
            # æœç´¢URLå‰å100ä¸ªå­—ç¬¦
            search_text = text[max(0, url_index-100):url_index+len(url)+100]

            for pattern in patterns:
                match = re.search(pattern, search_text, re.IGNORECASE)
                if match:
                    return match.group(1)

        return ''

    def _extract_title(self, text: str, url: str) -> str:
        """æå–èµ„æºæ ‡é¢˜"""
        # ç®€å•çš„æ ‡é¢˜æå–é€»è¾‘
        lines = text.split('\n')
        for line in lines:
            if url in line:
                # æ¸…ç†HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
                clean_line = re.sub(r'<[^>]+>', '', line)
                clean_line = re.sub(r'https?://\S+', '', clean_line)
                clean_line = clean_line.strip()
                if len(clean_line) > 5 and len(clean_line) < 100:
                    return clean_line[:50]  # é™åˆ¶é•¿åº¦

        return 'æœªçŸ¥èµ„æº'

    def _extract_size(self, text: str) -> str:
        """æå–æ–‡ä»¶å¤§å°"""
        size_patterns = [
            r'(\d+(?:\.\d+)?\s*[KMGT]B)',
            r'å¤§å°[:ï¼š]\s*(\d+(?:\.\d+)?\s*[KMGT]B)',
            r'(\d+(?:\.\d+)?\s*(?:MB|GB|TB))'
        ]

        for pattern in size_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1)

        return ''

    async def crawl_website(self, url: str, selector: str = None) -> List[Dict]:
        """çˆ¬å–æŒ‡å®šç½‘ç«™"""
        print(f"ğŸ•·ï¸ çˆ¬å–ç½‘ç«™: {url}")

        async with async_playwright() as p:
            browser = await p.firefox.launch(headless=True)
            page = await browser.new_page()

            try:
                await page.goto(url, timeout=30000)
                await page.wait_for_load_state('networkidle')

                # è·å–é¡µé¢å†…å®¹
                content = await page.content()
                soup = BeautifulSoup(content, 'html.parser')

                # æå–æ–‡æœ¬å†…å®¹
                if selector:
                    elements = soup.select(selector)
                    texts = [elem.get_text() for elem in elements]
                else:
                    texts = [soup.get_text()]

                # ä»æ–‡æœ¬ä¸­æå–ç½‘ç›˜ä¿¡æ¯
                results = []
                for text in texts:
                    results.extend(self.extract_netdisk_info(text))

                print(f"âœ… ä» {url} æå–åˆ° {len(results)} ä¸ªèµ„æº")
                return results

            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥ {url}: {e}")
                return []
            finally:
                await browser.close()

    async def crawl_search_engine(self, keywords: List[str], max_pages: int = 5):
        """çˆ¬å–ç½‘ç›˜æœç´¢å¼•æ“"""
        # è¿™é‡Œæ·»åŠ å…·ä½“çš„æœç´¢å¼•æ“çˆ¬å–é€»è¾‘
        # ä¾‹å¦‚ï¼šç›˜æœæœã€å°ç™½ç›˜ç­‰
        pass

    def save_to_csv(self, filename: str = None):
        """ä¿å­˜ç»“æœåˆ°CSV"""
        if not filename:
            filename = f"netdisk_resources_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        df = pd.DataFrame(self.results)
        if not df.empty:
            # å»é‡
            df = df.drop_duplicates(subset=['url'])

            # æ’åº
            df = df.sort_values(['platform', 'crawl_time'])

            # ä¿å­˜
            df.to_csv(filename, index=False, encoding='utf-8-sig')
            print(f"ğŸ“Š å·²ä¿å­˜ {len(df)} ä¸ªèµ„æºåˆ° {filename}")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            platform_stats = df['platform'].value_counts()
            print("ğŸ“ˆ å¹³å°ç»Ÿè®¡:")
            for platform, count in platform_stats.items():
                print(f"  {platform}: {count} ä¸ªèµ„æº")

        return filename

    async def run_spider(self, config: Dict):
        """è¿è¡Œçˆ¬è™«"""
        print("ğŸš€ å¯åŠ¨ç½‘ç›˜èµ„æºçˆ¬è™«...")

        # çˆ¬å–é…ç½®çš„ç½‘ç«™
        for site in config.get('websites', []):
            results = await self.crawl_website(
                site['url'],
                site.get('selector')
            )
            self.results.extend(results)

            # å»¶è¿Ÿé¿å…è¢«å°
            await asyncio.sleep(config.get('delay', 2))

        # ä¿å­˜ç»“æœ
        filename = self.save_to_csv()
        return filename


async def main():
    """ä¸»å‡½æ•°"""
    spider = NetdiskSpider()

    # çˆ¬è™«é…ç½®
    config = {
        'websites': [
            # ç¤ºä¾‹ç½‘ç«™ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
            {
                'url': 'https://example-resource-site.com',
                'selector': '.resource-item'
            }
        ],
        'delay': 3  # è¯·æ±‚é—´éš”ç§’æ•°
    }

    filename = await spider.run_spider(config)
    print(f"ğŸ‰ çˆ¬è™«å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {filename}")


if __name__ == "__main__":
    asyncio.run(main())