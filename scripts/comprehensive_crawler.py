#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å®šç½‘ç«™çˆ¬è™«ç¤ºä¾‹
é’ˆå¯¹å¸¸è§çš„ç½‘ç›˜èµ„æºåˆ†äº«ç½‘ç«™
"""

import asyncio
import re
from netdisk_spider import NetdiskSpider


class SpecificSiteCrawler(NetdiskSpider):
    """ç‰¹å®šç½‘ç«™çˆ¬è™«"""

    def __init__(self):
        super().__init__()

    async def crawl_telegram_channels(self, channels: list):
        """çˆ¬å–å…¬å¼€çš„Telegramé¢‘é“"""
        for channel in channels:
            print(f"ğŸ” çˆ¬å–Telegramé¢‘é“: {channel}")

            # Telegram Webç‰ˆé“¾æ¥
            url = f"https://t.me/s/{channel}"

            try:
                results = await self.crawl_website(url, ".tgme_widget_message_text")
                self.results.extend(results)
                await asyncio.sleep(3)
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥ {channel}: {e}")

    async def crawl_resource_forums(self):
        """çˆ¬å–èµ„æºè®ºå›ï¼ˆç¤ºä¾‹ï¼‰"""
        forums = [
            # ç¤ºä¾‹è®ºå›ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
            {
                'name': 'ç¤ºä¾‹è®ºå›1',
                'url': 'https://example-forum1.com/resources',
                'selector': '.post-content'
            },
            {
                'name': 'ç¤ºä¾‹è®ºå›2',
                'url': 'https://example-forum2.com/share',
                'selector': '.resource-item'
            }
        ]

        for forum in forums:
            print(f"ğŸ” çˆ¬å–è®ºå›: {forum['name']}")
            try:
                results = await self.crawl_website(forum['url'], forum['selector'])
                self.results.extend(results)
                await asyncio.sleep(5)  # è®ºå›é€šå¸¸éœ€è¦æ›´é•¿çš„é—´éš”
            except Exception as e:
                print(f"âŒ çˆ¬å–å¤±è´¥ {forum['name']}: {e}")

    async def crawl_baidu_tieba(self, tieba_names: list, max_pages: int = 5):
        """çˆ¬å–ç™¾åº¦è´´å§èµ„æºè´´"""
        for tieba_name in tieba_names:
            print(f"ğŸ” çˆ¬å–è´´å§: {tieba_name}")

            for page in range(1, max_pages + 1):
                url = f"https://tieba.baidu.com/f?kw={tieba_name}&pn={page*50}"

                try:
                    # çˆ¬å–è´´å§å¸–å­åˆ—è¡¨
                    results = await self.crawl_website(url, ".t_con")
                    self.results.extend(results)
                    await asyncio.sleep(2)
                except Exception as e:
                    print(f"âŒ çˆ¬å–å¤±è´¥ {tieba_name} ç¬¬{page}é¡µ: {e}")
                    break

    def create_search_keywords_config(self):
        """åˆ›å»ºæœç´¢å…³é”®è¯é…ç½®"""
        return {
            'å½±è§†èµ„æº': [
                '4Kç”µå½±', 'é«˜æ¸…ç”µå½±', 'è“å…‰åŸç›˜', 'ç”µè§†å‰§', 'çºªå½•ç‰‡',
                'åŠ¨æ¼«', 'ç»¼è‰º', 'æœ€æ–°ç”µå½±', 'ç»å…¸ç”µå½±'
            ],
            'å­¦ä¹ èµ„æº': [
                'ç¼–ç¨‹æ•™ç¨‹', 'Pythonæ•™ç¨‹', 'Webå¼€å‘', 'æ•°æ®ç§‘å­¦',
                'è€ƒç ”èµ„æ–™', 'è‹±è¯­å­¦ä¹ ', 'èŒåœºæŠ€èƒ½', 'è®¾è®¡æ•™ç¨‹'
            ],
            'è½¯ä»¶å·¥å…·': [
                'Adobeå…¨å®¶æ¡¶', 'Officeå¥—ä»¶', 'å¼€å‘å·¥å…·', 'ç³»ç»Ÿå·¥å…·',
                'è®¾è®¡è½¯ä»¶', 'éŸ³è§†é¢‘å·¥å…·', 'ç»¿è‰²è½¯ä»¶'
            ],
            'å…¶ä»–èµ„æº': [
                'éŸ³ä¹ä¸“è¾‘', 'ç”µå­ä¹¦', 'ç´ æèµ„æº', 'å­—ä½“åº“',
                'æ¸¸æˆ', 'æ‰‹æœºåº”ç”¨', 'æºç '
            ]
        }


async def run_comprehensive_crawl():
    """è¿è¡Œç»¼åˆçˆ¬è™«"""
    crawler = SpecificSiteCrawler()

    print("ğŸš€ å¼€å§‹ç»¼åˆèµ„æºçˆ¬å–...")

    # 1. çˆ¬å–å…¬å¼€Telegramé¢‘é“
    telegram_channels = [
        # ç¤ºä¾‹é¢‘é“ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
        'example_resources',
        'example_movies',
        'example_software'
    ]

    if telegram_channels:
        print("ğŸ“± çˆ¬å–Telegramé¢‘é“...")
        await crawler.crawl_telegram_channels(telegram_channels)

    # 2. çˆ¬å–ç™¾åº¦è´´å§
    tieba_names = [
        # ç¤ºä¾‹è´´å§ï¼ˆéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
        'èµ„æºåˆ†äº«', 'ç”µå½±åˆ†äº«', 'è½¯ä»¶åˆ†äº«'
    ]

    if tieba_names:
        print("ğŸ» çˆ¬å–ç™¾åº¦è´´å§...")
        await crawler.crawl_baidu_tieba(tieba_names, max_pages=3)

    # 3. çˆ¬å–èµ„æºè®ºå›
    print("ğŸ›ï¸ çˆ¬å–èµ„æºè®ºå›...")
    await crawler.crawl_resource_forums()

    # ä¿å­˜ç»“æœ
    filename = crawler.save_to_csv()

    print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(crawler.results)} ä¸ªèµ„æº")
    print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {filename}")

    return filename


if __name__ == "__main__":
    # è¿è¡Œçˆ¬è™«
    filename = asyncio.run(run_comprehensive_crawl())

    print(f"\nğŸ’¡ æ¥ä¸‹æ¥å¯ä»¥è¿è¡Œ:")
    print(f"python scripts/batch_transfer.py {filename} 10")